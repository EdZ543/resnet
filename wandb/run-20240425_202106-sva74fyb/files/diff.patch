diff --git a/dataloaders.py b/dataloaders.py
deleted file mode 100644
index 104b7ce..0000000
--- a/dataloaders.py
+++ /dev/null
@@ -1,31 +0,0 @@
-from torchvision import datasets, transforms
-from torch.utils.data import DataLoader
-
-transform = transforms.Compose(
-    [
-        transforms.ToTensor(),
-        # Paper uses per-pixel mean subtraction
-        # I use PyTorch's normalization
-        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
-        transforms.RandomCrop(32, padding=4),
-    ]
-)
-
-
-def get_data_loaders(batch_size):
-    """Get train and test data loaders for CIFAR-10 dataset."""
-
-    # Get data
-    training_data = datasets.CIFAR10(
-        root="data", train=True, download=True, transform=transform
-    )
-
-    test_data = datasets.CIFAR10(
-        root="data", train=False, download=True, transform=transform
-    )
-
-    # Create data loaders
-    train_dataloader = DataLoader(training_data, batch_size=batch_size)
-    test_dataloader = DataLoader(test_data, batch_size=batch_size)
-
-    return train_dataloader, test_dataloader
diff --git a/main.ipynb b/main.ipynb
index 5d094b9..9d8a455 100644
--- a/main.ipynb
+++ b/main.ipynb
@@ -2,14 +2,76 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [],
    "source": [
-    "import torch.nn as nn\n",
-    "import torch.nn.functional as F\n",
-    "from torch.utils.data import DataLoader\n",
-    "from torchvision import datasets, transforms"
+    "import os\n",
+    "import random\n",
+    "\n",
+    "import numpy as np\n",
+    "import torch\n",
+    "from tqdm.auto import tqdm\n",
+    "\n",
+    "# Ensure deterministic behavior\n",
+    "torch.backends.cudnn.deterministic = True\n",
+    "random.seed(42)\n",
+    "np.random.seed(42)\n",
+    "torch.manual_seed(42)\n",
+    "torch.cuda.manual_seed(42)\n",
+    "\n",
+    "# Device configuration\n",
+    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/eddiezhuang/.netrc\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "True"
+      ]
+     },
+     "execution_count": 3,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "import wandb\n",
+    "\n",
+    "wandb.login()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "wandb.init(\n",
+    "    project=\"resnet\",\n",
+    "    config={\n",
+    "        \"weight_decay\": 1e-4,\n",
+    "        \"momentum\": 0.9,\n",
+    "        \"batch_size\": 128,\n",
+    "        \"learning_rate\": 0.1,\n",
+    "        \"iterations\": 64000.\n",
+    "        \"n\": 3\n",
+    "    }\n",
+    ")"
    ]
   }
  ],
